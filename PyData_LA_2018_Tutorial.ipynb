{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyData_LA_2018_Tutorial.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "nRcFjrKQBKjs",
        "sBHis-I6AW6j",
        "0iFBq8iNg30L",
        "ThFjgOObBf9B",
        "K2Tou1jnBgFP",
        "aFR3se7N5MZR",
        "BfrkICrsekce",
        "YW9CMoYNVgd1",
        "9WVM2ihljviV",
        "zvI-0HR6brBZ"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "wwdOj9Cl6Zto",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Applying statistical modeling and machine learning to perform time-series forecasting."
      ]
    },
    {
      "metadata": {
        "id": "n2xCjl9O7K6x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Tamara Louie  \n",
        "PyData LA  \n",
        "October 2018"
      ]
    },
    {
      "metadata": {
        "id": "Y8H5Fx9L79Z9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Time-series data due diligence"
      ]
    },
    {
      "metadata": {
        "id": "cUqIIFFNcl6s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## How do we get some data?\n",
        "I think that this first step is very important, as it will determine if I can even tackle the problem using modeling or machine learning. I tend to choose modeling and machine learning problems where there is a wealth of readily available data, or the capability to create the data myself.\n",
        "\n",
        "I am planning on using data from this [link](http://insideairbnb.com/get-the-data.html), which appears to be from public data available about Airbnb.\n",
        "\n",
        "Before I start figuring out how to ingest this data, I first want to read about this data and understand if it is what I want, and if it fulfills some basic requirements about data quality."
      ]
    },
    {
      "metadata": {
        "id": "ZFqFK76KANmd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## How is the data is generated?\n",
        "There is a not a lot of documentation for this data.  Ideally I would look more into how this data was generated.\n",
        "\n",
        "**Where did the data come from? Has it been processed or modified in any way?**\n",
        "*   I do not know exactly where the data comes from. \n",
        "*   I do not know if there has been additional processing or modification.  \n",
        "  \n",
        "  \n",
        "**Is the data clean or dirty? Is additional processing necessary (e.g., accounting for nulls, changing units, etc.)?**\n",
        "*   Yes, lack of entries for some dates.\n",
        "*   Yes, need to change the dates to date type."
      ]
    },
    {
      "metadata": {
        "id": "mnwfl8cYAsjr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Where do I store my data? How do I access my data?\n",
        "This aspect is quite time intensive if your data is large and / or not in a state to be ingested by Python. In fact, this step may take the majority of your time when you are performing a modeling or machine learning task, between getting and reading in the data, cleaning data, and making sure the data is being ingested correctly, and verifying that the data is being logged correctly.\n",
        "\n",
        "How do we load it into a place where we can access it? How do we access it without crashing? Do I need to sample my data? Can I load it all into memory on a single machine? \n",
        "\n",
        "Rarely does my data ever come in a CSV file. Rarely does it all fit in memory on a single machine. However, in this case we have both of these things.  So let us **download the data** from this **[link](http://insideairbnb.com/get-the-data.html)**, specifically the \"Los Angeles, California, United States\" section, **reviews.csv ONLY** (this file needs 23MB space on your local drive)."
      ]
    },
    {
      "metadata": {
        "id": "nRcFjrKQBKjs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Read in my data\n",
        "**Please download from http://insideairbnb.com/get-the-data.html the following CSV file under the \"Los Angeles\" section:**\n",
        "\n",
        "\treviews.csv\n",
        "  \n",
        "\n",
        "You should have **reviews.csv** stored somewhere locally. \n",
        "\n",
        "Run the cell below, and click on the button **Choose Files**, to select your **reviews.csv** to upload.  \n",
        "\n",
        "*Note: It will take some time to upload this data, so please start your upload and this upload will run for a couple of minutes.*"
      ]
    },
    {
      "metadata": {
        "id": "Ur80thmY8BsB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sBHis-I6AW6j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import some relevant packages\n",
        "Ideally, one should set up their own virtual environment and determine the versions of each library that they are using.  Here, we will assume that the colaboratory environment has some shared environment with access to common Python libraries and the ability to install other libraries necessary."
      ]
    },
    {
      "metadata": {
        "id": "f1JOuRDd-vGA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import display, HTML, display_html\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "\n",
        "\n",
        "# set formatting\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# read in CSV file data\n",
        "# df = pd.read_csv('train_2_sample.csv')\n",
        "df = pd.read_csv('reviews.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0iFBq8iNg30L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Look at my data\n",
        "- Looks like there are 64,015 rows and 84 columns in this dataset.  \n",
        "- All of the columns contain floats.\n",
        "- **The data is NOT complete (i.e., there are nulls).  We have to either infer a zero or some other value.**\n",
        "\n",
        "**What are some caveats to the data?**   \n",
        "**What do these columns mean?**"
      ]
    },
    {
      "metadata": {
        "id": "2Jn55hjsBMCj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# look at data\n",
        "display(df.head())\n",
        "\n",
        "# look a shape of data\n",
        "display(df.shape)\n",
        "\n",
        "# display descriptive statistics\n",
        "display(df.describe(percentiles=[0.25,0.5,0.75,0.85,0.95,0.99]))\n",
        "\n",
        "# look at data types. Ideally look at all rows. Only look at first five here for minimal output.\n",
        "display(df.iloc[:5,:5].dtypes)\n",
        "\n",
        "# see if any columns have nulls. Ideally look at all rows. Only look at first five here for minimal output.\n",
        "display(df.iloc[:5,:5].isnull().any())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LPTWaJHCd5n3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What are some questions I can answer with this data?\n",
        "\n",
        "Understand the limitations of your data and what potential questions can be answered by data is important.  These questions can reduce, expand, or modify the scope of your project.\n",
        "\n",
        "If you defined a scope or goal for your project before digging into the data, this might be a good time to re-visit it.\n",
        "\n",
        "**Data.** We have daily count of reviews for given listing ids for given dates.\n",
        "\n",
        "**Questions I could try to answer. **\n",
        "\n",
        "*   Forecast future number of reviews for the Los Angeles area.\n",
        "* Forecast the future number of reviews for specific listings in the Los Angeles area."
      ]
    },
    {
      "metadata": {
        "id": "-DnHNENgg5g5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What techniques may help answer these questions?\n",
        "### Statistical models\n",
        "*   **Ignore the time-series aspect completely and model using traditional statistical modeling toolbox.** \n",
        "  *   *Examples.* Regression-based models.  \n",
        "  *   *Notes.* We will discuss why this idea might be bad for modeling. [Link to Duke site on forecasting](https://people.duke.edu/~rnau/411home.htm).\n",
        "*   **Univariate statistical time-series modeling.**\n",
        "  *   *Examples.* Averaging and smoothing models, ARIMA models.\n",
        "*   **Slight modifications to univariate statistical time-series modeling.**\n",
        "  *    *Examples.* External regressors, multi-variate models.\n",
        "*   **Additive or component models.**\n",
        "  *  *Examples.* Facebook Prophet package.\n",
        "*   **Structural time series modeling.**\n",
        "  *    *Examples.* Bayesian structural time series modeling, hierarchical time series modeling.\n",
        "\n",
        "### Machine learning models\n",
        "\n",
        "*   **Ignore the time-series aspect completely and model using traditional machine learning modeling toolbox.** \n",
        "  *   *Examples.* Support Vector Machines (SVMs), Random Forest Regression, Gradient-Boosted Decision Trees (GBDTs).\n",
        "  *   *Notes.* We will discuss why this idea might be bad for modeling.\n",
        "*   **Hidden markov models (HMMs).**\n",
        "  *   *Examples.* \n",
        "*   **Other sequence-based models.**\n",
        "  *    *Examples.* \n",
        "*   **Gaussian processes (GPs).**\n",
        "  *    *Examples.* \n",
        "*   ** Recurrent neural networks (RNNs).**\n",
        "  *  *Examples.* \n",
        "  \n",
        "### Additional data considerations before choosing a model\n",
        "*   Whether or not to incorporate external data\n",
        "*   Whether or not to keep as univariate or multi-variate (i.e., which features and number of features)\n",
        "*   Outlier detection and removal\n",
        "*   Missing value imputation"
      ]
    },
    {
      "metadata": {
        "id": "ceK93q2WBtHm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Let's analyze some time-series data!\n",
        "\n",
        "- [Link 1](https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/)\n",
        "- [Link 2](https://content.nexosis.com/blog/methods-of-demand-forecasting-bsts-prophet)"
      ]
    },
    {
      "metadata": {
        "id": "ThFjgOObBf9B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Process my data"
      ]
    },
    {
      "metadata": {
        "id": "07ZLKaGFF4-C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Rename columns\n",
        "df = df.rename(columns = {'date': 'ds', 'listing_id': 'ts'})\n",
        "\n",
        "# Group data by number of listings per date\n",
        "df_example = df.groupby(by = 'ds').agg({'ts': 'count'})\n",
        "\n",
        "# Change index to datetime\n",
        "df_example.index = pd.to_datetime(df_example.index)\n",
        "\n",
        "# Set frequency of time series\n",
        "df_example = df_example.asfreq(freq='1D')\n",
        "\n",
        "# Sort the values\n",
        "df_example = df_example.sort_index(ascending = True)\n",
        "\n",
        "# Fill values with 0\n",
        "df_example = df_example.fillna(value = 0)\n",
        "\n",
        "# Show the end of the data\n",
        "display(df_example.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K2Tou1jnBgFP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Plot my data\n",
        "- There does appear to be an overall increasing trend. \n",
        "- There appears to be some differences in the variance over time. \n",
        "- There may be some seasonality (i.e., cycles) in the data.\n",
        "- Not sure about outliers."
      ]
    },
    {
      "metadata": {
        "id": "1guVcTye4P74",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot time series data\n",
        "f, ax = plt.subplots(1,1)\n",
        "ax.plot(df_example['ts'])\n",
        "\n",
        "# Add title\n",
        "ax.set_title('Time-series graph for 1 time-series example')\n",
        "\n",
        "# Rotate x-labels\n",
        "ax.tick_params(axis = 'x', rotation = 45)\n",
        "\n",
        "# Show graph\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X6BxNlJ-CWa1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Random Walk\n",
        "\n",
        "https://people.duke.edu/~rnau/Notes_on_the_random_walk_model--Robert_Nau.pdf\n",
        "https://people.duke.edu/~rnau/411rand.htm"
      ]
    },
    {
      "metadata": {
        "id": "aFR3se7N5MZR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Look at stationarity\n",
        "Most time-series models assume that the underlying time-series data is **stationary**.  This assumption gives us some nice statistical properties that allows us to use various models for forecasting.\n",
        "\n",
        "**Stationarity** is a staistical assumption that a time-series has:\n",
        "*   **Constant mean**\n",
        "*   **Constant variance**\n",
        "*   **Autocovariance does not depend on time**\n",
        "\n",
        "More simply put, if we are using past data to predict future data, we should assume that the data will follow the same general trends and patterns as in the past.  This general statement holds for most training data and modeling tasks.\n",
        "\n",
        "**There are some good diagrams and explanations on stationarity [here](https://www.analyticsvidhya.com/blog/2015/12/complete-tutorial-time-series-modeling/) and [here](https://people.duke.edu/~rnau/411diff.htm).**\n",
        "\n",
        "Sometimes we need to transform the data in order to make it stationary.  However, this  transformation then calls into question if this data is truly stationary and is suited to be modeled using these techniques.\n",
        "\n",
        "**Looking at our data:**\n",
        "- Rolling mean and standard deviation look like they change over time.  There may be some de-trending and removing seasonality involved. Based on **Dickey-Fuller test**, because p = 0.07, we fail to reject the null hypothesis (that the time series is not stationary) at the p = 0.32 level, thus concluding that we fail to reject the null hypothesis that our **time series is not stationary**."
      ]
    },
    {
      "metadata": {
        "id": "RG2UQ8kK7oUJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "def test_stationarity(df, ts):\n",
        "    \"\"\"\n",
        "    Test stationarity using moving average statistics and Dickey-Fuller test\n",
        "    Source: https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/\n",
        "    \"\"\"\n",
        "    \n",
        "    # Determing rolling statistics\n",
        "    rolmean = df[ts].rolling(window = 12, center = False).mean()\n",
        "    rolstd = df[ts].rolling(window = 12, center = False).std()\n",
        "    \n",
        "    # Plot rolling statistics:\n",
        "    orig = plt.plot(df[ts], \n",
        "                    color = 'blue', \n",
        "                    label = 'Original')\n",
        "    mean = plt.plot(rolmean, \n",
        "                    color = 'red', \n",
        "                    label = 'Rolling Mean')\n",
        "    std = plt.plot(rolstd, \n",
        "                   color = 'black', \n",
        "                   label = 'Rolling Std')\n",
        "    plt.legend(loc = 'best')\n",
        "    plt.title('Rolling Mean & Standard Deviation for %s' %(ts))\n",
        "    plt.xticks(rotation = 45)\n",
        "    plt.show(block = False)\n",
        "    plt.close()\n",
        "    \n",
        "    # Perform Dickey-Fuller test:\n",
        "    # Null Hypothesis (H_0): time series is not stationary\n",
        "    # Alternate Hypothesis (H_1): time series is stationary\n",
        "    print 'Results of Dickey-Fuller Test:'\n",
        "    dftest = adfuller(df[ts], \n",
        "                      autolag='AIC')\n",
        "    dfoutput = pd.Series(dftest[0:4], \n",
        "                         index = ['Test Statistic',\n",
        "                                  'p-value',\n",
        "                                  '# Lags Used',\n",
        "                                  'Number of Observations Used'])\n",
        "    for key, value in dftest[4].items():\n",
        "        dfoutput['Critical Value (%s)'%key] = value\n",
        "    print dfoutput"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tbziac6t7r_d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_stationarity(df = df_example, ts = 'ts')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jK6ldw3J-Hoe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Correct for stationarity\n",
        "\n",
        "It is common for time series data to have to correct for non-stationarity. \n",
        "\n",
        "2 common reasons behind non-stationarity are:\n",
        "\n",
        "1. **Trend** – mean is not constant over time.\n",
        "2. **Seasonality** – variance is not constant over time.\n",
        "\n",
        "There are ways to correct for trend and seasonality, to make the time series stationary.\n",
        "\n",
        "I will include it below (commented). However, it is not necessary for our specific time series example."
      ]
    },
    {
      "metadata": {
        "id": "PiFJF8dl-65Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Eliminating trend and seasonality\n",
        "*   **Transformation**\n",
        "  *   *Examples.* Log, square root, etc.\n",
        "  *   We are going to look at log.\n",
        "*   **Smoothing**\n",
        "  *  *Examples.* Weekly average, monthly average, rolling averages.\n",
        "  *   We are going to look at weekly average.\n",
        "*   **Differencing**\n",
        "  *   We are going to look at first-order differencing.\n",
        "*   **Polynomial Fitting**\n",
        "  *  *Examples.* Fit a regression model.\n",
        "*   **Decomposition**"
      ]
    },
    {
      "metadata": {
        "id": "BfrkICrsekce",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Transformation, Smoothing, and Differencing\n",
        "**Looking at our data:**\n",
        "- Applying log transformation, weekly moving average smoothing, and differencing made the data more stationary over time. Based on **Dickey-Fuller test**, because p = < 0.05, we fail to reject the null hypothesis (that the time series is not stationary) at the p = 0.05 level, thus concluding that the **time series is stationary**."
      ]
    },
    {
      "metadata": {
        "id": "YVCmLXNfKLBh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_transformed_data(df, ts, ts_transform):\n",
        "  \"\"\"\n",
        "  Plot transformed and original time series data\n",
        "  \"\"\"\n",
        "  # Plot time series data\n",
        "  f, ax = plt.subplots(1,1)\n",
        "  ax.plot(df[ts])\n",
        "  ax.plot(df[ts_transform], color = 'red')\n",
        "\n",
        "  # Add title\n",
        "  ax.set_title('%s and %s time-series graph' %(ts, ts_transform))\n",
        "\n",
        "  # Rotate x-labels\n",
        "  ax.tick_params(axis = 'x', rotation = 45)\n",
        "\n",
        "  # Add legend\n",
        "  ax.legend([ts, ts_transform])\n",
        "  \n",
        "  plt.show()\n",
        "  plt.close()\n",
        "  \n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qx_6oozH-4_L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Transformation - log ts\n",
        "df_example['ts_log'] = df_example['ts'].apply(lambda x: np.log(x))\n",
        "\n",
        "# Transformation - 7-day moving averages of log ts\n",
        "df_example['ts_log_moving_avg'] = df_example['ts_log'].rolling(window = 7,\n",
        "                                                               center = False).mean()\n",
        "\n",
        "# Transformation - 7-day moving average ts\n",
        "df_example['ts_moving_avg'] = df_example['ts'].rolling(window = 7,\n",
        "                                                       center = False).mean()\n",
        "\n",
        "# Transformation - Difference between logged ts and first-order difference logged ts\n",
        "# df_example['ts_log_diff'] = df_example['ts_log'] - df_example['ts_log'].shift()\n",
        "df_example['ts_log_diff'] = df_example['ts_log'].diff()\n",
        "\n",
        "# Transformation - Difference between ts and moving average ts\n",
        "df_example['ts_moving_avg_diff'] = df_example['ts'] - df_example['ts_moving_avg']\n",
        "\n",
        "# Transformation - Difference between logged ts and logged moving average ts\n",
        "df_example['ts_log_moving_avg_diff'] = df_example['ts_log'] - df_example['ts_log_moving_avg']\n",
        "\n",
        "# Transformation - Difference between logged ts and logged moving average ts\n",
        "df_example_transform = df_example.dropna()\n",
        "\n",
        "# Transformation - Logged exponentially weighted moving averages (EWMA) ts\n",
        "df_example_transform['ts_log_ewma'] = df_example_transform['ts_log'].ewm(halflife = 7,\n",
        "                                                                         ignore_na = False,\n",
        "                                                                         min_periods = 0,\n",
        "                                                                         adjust = True).mean()\n",
        "\n",
        "# Transformation - Difference between logged ts and logged EWMA ts\n",
        "df_example_transform['ts_log_ewma_diff'] = df_example_transform['ts_log'] - df_example_transform['ts_log_ewma']\n",
        "\n",
        "# Display data\n",
        "display(df_example_transform.head())\n",
        "\n",
        "# Plot data\n",
        "plot_transformed_data(df = df_example, \n",
        "                      ts = 'ts', \n",
        "                      ts_transform = 'ts_log')\n",
        "# Plot data\n",
        "plot_transformed_data(df = df_example, \n",
        "                      ts = 'ts_log', \n",
        "                      ts_transform = 'ts_log_moving_avg')\n",
        "\n",
        "# Plot data\n",
        "plot_transformed_data(df = df_example_transform, \n",
        "                      ts = 'ts', \n",
        "                      ts_transform = 'ts_moving_avg')\n",
        "\n",
        "# Plot data\n",
        "plot_transformed_data(df = df_example_transform, \n",
        "                      ts = 'ts_log', \n",
        "                      ts_transform = 'ts_log_diff')\n",
        "\n",
        "# Plot data\n",
        "plot_transformed_data(df = df_example_transform, \n",
        "                      ts = 'ts', \n",
        "                      ts_transform = 'ts_moving_avg_diff')\n",
        "\n",
        "# Plot data\n",
        "plot_transformed_data(df = df_example_transform, \n",
        "                      ts = 'ts_log', \n",
        "                      ts_transform = 'ts_log_moving_avg_diff')\n",
        "\n",
        "# Plot data\n",
        "plot_transformed_data(df = df_example_transform, \n",
        "                      ts = 'ts_log', \n",
        "                      ts_transform = 'ts_log_ewma')\n",
        "\n",
        "# Plot data\n",
        "plot_transformed_data(df = df_example_transform, \n",
        "                      ts = 'ts_log', \n",
        "                      ts_transform = 'ts_log_ewma_diff')\n",
        "\n",
        "# Perform stationarity test\n",
        "test_stationarity(df = df_example_transform, \n",
        "                  ts = 'ts_log')\n",
        "\n",
        "# Perform stationarity test\n",
        "test_stationarity(df = df_example_transform, \n",
        "                  ts = 'ts_moving_avg')\n",
        "\n",
        "# Perform stationarity test\n",
        "test_stationarity(df = df_example_transform, \n",
        "                  ts = 'ts_log_moving_avg')\n",
        "\n",
        "# Perform stationarity test\n",
        "test_stationarity(df = df_example_transform,\n",
        "                  ts = 'ts_log_diff')\n",
        "\n",
        "# Perform stationarity test\n",
        "test_stationarity(df = df_example_transform,\n",
        "                  ts = 'ts_moving_avg_diff')\n",
        "\n",
        "# Perform stationarity test\n",
        "test_stationarity(df = df_example_transform,\n",
        "                  ts = 'ts_log_moving_avg_diff')\n",
        "\n",
        "# Perform stationarity test\n",
        "test_stationarity(df = df_example_transform, \n",
        "                  ts = 'ts_log_ewma')\n",
        "\n",
        "# Perform stationarity test\n",
        "test_stationarity(df = df_example_transform,\n",
        "                  ts = 'ts_log_ewma_diff')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YW9CMoYNVgd1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Decomposition: trend, seasonality, residuals\n",
        "**Looking at our data:**\n",
        "- De-trending and de-seasonalizing made the data (i.e., the residuals) more stationary over time. Based on **Dickey-Fuller test**, because p = < 0.05, we fail to reject the null hypothesis (that the time series is not stationary) at the p = 0.05 level, thus concluding that the **time series is stationary**."
      ]
    },
    {
      "metadata": {
        "id": "fA-rG88qVpdF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_decomposition(df, ts, trend, seasonal, residual):\n",
        "  \"\"\"\n",
        "  Plot time series data\n",
        "  \"\"\"\n",
        "  f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize = (15, 5), sharex = True)\n",
        "\n",
        "  ax1.plot(df[ts], label = 'Original')\n",
        "  ax1.legend(loc = 'best')\n",
        "  ax1.tick_params(axis = 'x', rotation = 45)\n",
        "\n",
        "  ax2.plot(df[trend], label = 'Trend')\n",
        "  ax2.legend(loc = 'best')\n",
        "  ax2.tick_params(axis = 'x', rotation = 45)\n",
        "\n",
        "  ax3.plot(df[seasonal],label = 'Seasonality')\n",
        "  ax3.legend(loc = 'best')\n",
        "  ax3.tick_params(axis = 'x', rotation = 45)\n",
        "\n",
        "  ax4.plot(df[residual], label = 'Residuals')\n",
        "  ax4.legend(loc = 'best')\n",
        "  ax4.tick_params(axis = 'x', rotation = 45)\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Show graph\n",
        "  plt.suptitle('Trend, Seasonal, and Residual Decomposition of %s' %(ts), \n",
        "               x = 0.5, \n",
        "               y = 1.05, \n",
        "               fontsize = 18)\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "  \n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "voOjEMD9gKAS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "decomposition = seasonal_decompose(df_example_transform['ts_log'], freq = 365)\n",
        "\n",
        "df_example_transform.loc[:,'trend'] = decomposition.trend\n",
        "df_example_transform.loc[:,'seasonal'] = decomposition.seasonal\n",
        "df_example_transform.loc[:,'residual'] = decomposition.resid\n",
        "\n",
        "plot_decomposition(df = df_example_transform, \n",
        "                   ts = 'ts_log', \n",
        "                   trend = 'trend',\n",
        "                   seasonal = 'seasonal', \n",
        "                   residual = 'residual')\n",
        "\n",
        "test_stationarity(df = df_example_transform.dropna(), ts = 'residual')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AbTlhWommsXE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Let us model some time-series data! Finally! ARIMA models.\n",
        "\n",
        "We will be doing an example here! We can use ARIMA models when we know there is dependence between values and we can leverage that information to forecast.\n",
        "\n",
        "**ARIMA = Auto-Regressive Integrated Moving Average**.   \n",
        "**Assumptions.** The time-series is stationary.  \n",
        "**Depend on:**  \n",
        "  **1. Number of AR (Auto-Regressive) terms (p).**  \n",
        "  **2. Number of MA (Moving Average) terms (q):**  \n",
        "  **3. Number of Difference terms (d).**  "
      ]
    },
    {
      "metadata": {
        "id": "9WVM2ihljviV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ACF and PACF Plots\n",
        "**How do we determine p, d, and q?**\n",
        "For p and q, we can use ACF and PACF plots (below).\n",
        "\n",
        "**Autocorrelation Function (ACF).** Correlation between the time series with a lagged version of itself (e.g., correlation of Y(t) with Y(t-1)).\n",
        "\n",
        "**Partial Autocorrelation Function (PACF).** Additional correlation explained by each successive lagged term.\n",
        "\n",
        "**How do we interpret ACF and PACF plots?**\n",
        "- p – Lag value where the PACF chart crosses the upper confidence interval for the first time.\n",
        "- q – Lag value where the ACF chart crosses the upper confidence interval for the first time.\n"
      ]
    },
    {
      "metadata": {
        "id": "o0ea7gklHcwp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_acf_pacf(df, ts):\n",
        "  \"\"\"\n",
        "  Plot auto-correlation function (ACF) and partial auto-correlation (PACF) plots\n",
        "  \"\"\"\n",
        "  f, (ax1, ax2) = plt.subplots(1,2, figsize = (10, 5)) \n",
        "\n",
        "  #Plot ACF: \n",
        "\n",
        "  ax1.plot(lag_acf)\n",
        "  ax1.axhline(y=0,linestyle='--',color='gray')\n",
        "  ax1.axhline(y=-1.96/np.sqrt(len(df[ts])),linestyle='--',color='gray')\n",
        "  ax1.axhline(y=1.96/np.sqrt(len(df[ts])),linestyle='--',color='gray')\n",
        "  ax1.set_title('Autocorrelation Function for %s' %(ts))\n",
        "\n",
        "  #Plot PACF:\n",
        "  ax2.plot(lag_pacf)\n",
        "  ax2.axhline(y=0,linestyle='--',color='gray')\n",
        "  ax2.axhline(y=-1.96/np.sqrt(len(df[ts])),linestyle='--',color='gray')\n",
        "  ax2.axhline(y=1.96/np.sqrt(len(df[ts])),linestyle='--',color='gray')\n",
        "  ax2.set_title('Partial Autocorrelation Function for %s' %(ts))\n",
        "  \n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "  \n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vpMF8s7NkVjE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#ACF and PACF plots:\n",
        "from statsmodels.tsa.stattools import acf, pacf\n",
        "\n",
        "# determine ACF and PACF\n",
        "lag_acf = acf(np.array(df_example_transform['ts_log_diff']), nlags = 20)\n",
        "lag_pacf = pacf(np.array(df_example_transform['ts_log_diff']), nlags = 20)\n",
        "\n",
        "# plot ACF and PACF\n",
        "plot_acf_pacf(df = df_example_transform, ts = 'ts_log_diff')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1a_2PefcYouT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_arima_model(df, ts, p, d, q):\n",
        "  \"\"\"\n",
        "  Run ARIMA model\n",
        "  \"\"\"\n",
        "  from statsmodels.tsa.arima_model import ARIMA\n",
        "\n",
        "  # fit ARIMA model on time series\n",
        "  model = ARIMA(df[ts], order=(p, d, q))  \n",
        "  results_ = model.fit(disp=-1)  \n",
        "  \n",
        "  # get lengths correct to calculate RSS\n",
        "  len_results = len(results_.fittedvalues)\n",
        "  ts_modified = df[ts][-len_results:]\n",
        "  \n",
        "  # calculate root mean square error (RMSE) and residual sum of squares (RSS)\n",
        "  rss = sum((results_.fittedvalues - ts_modified)**2)\n",
        "  rmse = np.sqrt(rss / len(df[ts]))\n",
        "  # plot fit\n",
        "  plt.plot(df[ts])\n",
        "  plt.plot(results_.fittedvalues, color = 'red')\n",
        "  plt.title('For ARIMA model (%i, %i, %i) for ts %s, RSS: %.4f, RMSE: %.4f' %(p, d, q, ts, rss, rmse))\n",
        "  \n",
        "  plt.show()\n",
        "  plt.close()\n",
        "  \n",
        "  return results_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R1pHK8zAY_-z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Note: I do the differencing in the transformation of the data 'ts_log_diff'\n",
        "# AR model with 1st order differencing - ARIMA (1,1,0)\n",
        "model_AR = run_arima_model(df = df_example_transform, \n",
        "                           ts = 'ts_log_diff', \n",
        "                           p = 1, \n",
        "                           d = 0, \n",
        "                           q = 0)\n",
        "\n",
        "# MA model with 1st order differencing - ARIMA (0,1,1)\n",
        "model_MA = run_arima_model(df = df_example_transform, \n",
        "                           ts = 'ts_log_diff', \n",
        "                           p = 0, \n",
        "                           d = 0, \n",
        "                           q = 1)\n",
        "\n",
        "# ARMA model with 1st order differencing - ARIMA (1,1,1)\n",
        "model_MA = run_arima_model(df = df_example_transform, \n",
        "                           ts = 'ts_log_diff', \n",
        "                           p = 1, \n",
        "                           d = 0, \n",
        "                           q = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "feEXUUlXQHOe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Undoing the differencing\n",
        "# predictions_ARIMA_diff = pd.Series(model_ARIMA.fittedvalues, copy=True)\n",
        "# predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\n",
        "# test = np.exp(predictions_ARIMA_diff_cumsum)\n",
        "\n",
        "# # Undoing the logging\n",
        "# predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value = 0)\n",
        "# predictions_ARIMA = np.exp(predictions_ARIMA_log)\n",
        "\n",
        "# # Get RMSE\n",
        "# rss = sum((predictions_ARIMA - df_example_transform['ts'])**2)\n",
        "# rmse = np.sqrt(rss / len(df_example_transform['ts']))\n",
        "# # Plotting\n",
        "# plt.plot(df_example['ts'])\n",
        "# plt.plot(predictions_ARIMA)\n",
        "# # plt.title('RMSE: %.4f'% rmse))\n",
        "# plt.title('RMSE: %.4f'% np.sqrt(sum((test - df_example_transform['ts'])**2)/len(df_example['ts'])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vu0387ctTS93",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Let us model some time-series data! Finally! Facebook Prophet package.\n",
        "\n",
        "We will be doing an example here! Installing the necessary packages might take a couple of minutes.  In the meantime, I can talk a bit about [Facebook Prophet](https://facebook.github.io/prophet/), a tool that allows folks to forecast using additive or component models relatively easily.  It can also include things like:\n",
        "* Day of week effects\n",
        "* Day of year effects\n",
        "* Holiday effects\n",
        "* Trend trajectory\n",
        "* Can do MCMC sampling"
      ]
    },
    {
      "metadata": {
        "id": "qxK5LtVmTz28",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "outputId": "e30794c9-0015-4b32-933e-f3cd03e264e7"
      },
      "cell_type": "code",
      "source": [
        "!pip install pystan\n",
        "!pip install fbprophet\n",
        "from fbprophet import Prophet\n",
        "import datetime\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pystan\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/23/f414627f36a39998d06469f7faa79f2d940753d82adfd0a1bf9059c18355/pystan-2.18.0.0-cp27-cp27mu-manylinux1_x86_64.whl (50.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 50.0MB 310kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python2.7/dist-packages (from pystan) (1.14.6)\n",
            "Collecting Cython!=0.25.1,>=0.22 (from pystan)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/ae/be506d47f77fd31101a29eb740120f39050423f941cb0a461e4247330a47/Cython-0.28.5-cp27-cp27mu-manylinux1_x86_64.whl (3.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.3MB 6.1MB/s \n",
            "\u001b[?25hInstalling collected packages: Cython, pystan\n",
            "Successfully installed Cython-0.28.5 pystan-2.18.0.0\n",
            "Collecting fbprophet\n",
            "  Downloading https://files.pythonhosted.org/packages/83/a1/a39be1675a62597e4c0d4fdaeb65b14752b8aa8afc92e90edd91614353ab/fbprophet-0.3.post2.tar.gz\n",
            "Requirement already satisfied: Cython>=0.22 in /usr/local/lib/python2.7/dist-packages (from fbprophet) (0.28.5)\n",
            "Requirement already satisfied: pystan>=2.14 in /usr/local/lib/python2.7/dist-packages (from fbprophet) (2.18.0.0)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from fbprophet) (1.14.6)\n",
            "Requirement already satisfied: pandas>=0.20.1 in /usr/local/lib/python2.7/dist-packages (from fbprophet) (0.22.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from fbprophet) (2.1.2)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas>=0.20.1->fbprophet) (2018.5)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python2.7/dist-packages (from pandas>=0.20.1->fbprophet) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python2.7/dist-packages (from matplotlib>=2.0.0->fbprophet) (0.10.0)\n",
            "Requirement already satisfied: backports.functools-lru-cache in /usr/local/lib/python2.7/dist-packages (from matplotlib>=2.0.0->fbprophet) (1.5)\n",
            "Requirement already satisfied: subprocess32 in /usr/local/lib/python2.7/dist-packages (from matplotlib>=2.0.0->fbprophet) (3.5.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python2.7/dist-packages (from matplotlib>=2.0.0->fbprophet) (1.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python2.7/dist-packages (from matplotlib>=2.0.0->fbprophet) (2.2.1)\n",
            "Building wheels for collected packages: fbprophet\n",
            "  Running setup.py bdist_wheel for fbprophet ... \u001b[?25l-\b \b\\"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kelYeUjdVfbV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def days_between(d1, d2):\n",
        "    \"\"\"Calculate the number of days between two dates.  D1 is start date (inclusive) and d2 is end date (inclusive)\"\"\"\n",
        "    d1 = datetime.strptime(d1, \"%Y-%m-%d\")\n",
        "    d2 = datetime.strptime(d2, \"%Y-%m-%d\")\n",
        "    return abs((d2 - d1).days + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PMnVhgNkVMAm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Inputs for query\n",
        "\n",
        "date_column = 'dt'\n",
        "metric_column = 'ts'\n",
        "table = df_example\n",
        "start_training_date = '2010-07-03'\n",
        "end_training_date = '2018-09-08'\n",
        "start_forecasting_date = '2018-09-09'\n",
        "end_forecasting_date = '2018-12-31'\n",
        "year_to_estimate = '2018'\n",
        "\n",
        "# Inputs for forecasting\n",
        "\n",
        "# future_num_points\n",
        "# If doing different time intervals, change future_num_points\n",
        "future_num_points = days_between(start_forecasting_date, end_forecasting_date)\n",
        "\n",
        "cap = None # 2e6\n",
        "\n",
        "# growth: default = 'linear'\n",
        "# Can also choose 'logistic'\n",
        "growth = 'linear'\n",
        "\n",
        "# n_changepoints: default = 25, uniformly placed in first 80% of time series\n",
        "n_changepoints = 25 \n",
        "\n",
        "# changepoint_prior_scale: default = 0.05\n",
        "# Increasing it will make the trend more flexible\n",
        "changepoint_prior_scale = 0.05 \n",
        "\n",
        "# changpoints: example = ['2016-01-01']\n",
        "changepoints = None \n",
        "\n",
        "# holidays_prior_scale: default = 10\n",
        "# If you find that the holidays are overfitting, you can adjust their prior scale to smooth them\n",
        "holidays_prior_scale = 10 \n",
        "\n",
        "# interval_width: default = 0.8\n",
        "interval_width = 0.8 \n",
        "\n",
        "# mcmc_samples: default = 0\n",
        "# By default Prophet will only return uncertainty in the trend and observation noise.\n",
        "# To get uncertainty in seasonality, you must do full Bayesian sampling. \n",
        "# Replaces typical MAP estimation with MCMC sampling, and takes MUCH LONGER - e.g., 10 minutes instead of 10 seconds.\n",
        "# If you do full sampling, then you will see the uncertainty in seasonal components when you plot:\n",
        "mcmc_samples = 0\n",
        "\n",
        "# holiday: default = None\n",
        "# thanksgiving = pd.DataFrame({\n",
        "#   'holiday': 'thanksgiving',\n",
        "#   'ds': pd.to_datetime(['2014-11-27', '2015-11-26',\n",
        "#                         '2016-11-24', '2017-11-23']),\n",
        "#   'lower_window': 0,\n",
        "#   'upper_window': 4,\n",
        "# })\n",
        "# christmas = pd.DataFrame({\n",
        "#   'holiday': 'christmas',\n",
        "#   'ds': pd.to_datetime(['2014-12-25', '2015-12-25', \n",
        "#                         '2016-12-25','2017-12-25']),\n",
        "#   'lower_window': -1,\n",
        "#   'upper_window': 0,\n",
        "# })\n",
        "# holidays = pd.concat((thanksgiving,christmas))\n",
        "holidays = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DM_mns4lWC-M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# get relevant data - note: could also try this with ts_log_diff\n",
        "df_prophet = df_example_transform[['ts']]\n",
        "\n",
        "# reset index\n",
        "df_prophet = df_prophet.reset_index()\n",
        "\n",
        "# rename columns\n",
        "df_prophet = df_prophet.rename(columns = {'ds': 'ds', 'ts': 'y'})\n",
        "\n",
        "# Change 'ds' type from datetime to date (necessary for FB Prophet)\n",
        "df_prophet['ds'] = pd.to_datetime(df_prophet['ds'])\n",
        "\n",
        "# Change 'y' type to numeric (necessary for FB Prophet)\n",
        "df_prophet['y'] = pd.to_numeric(df_prophet['y'], errors='ignore')\n",
        "\n",
        "# Remove any outliers\n",
        "# df.loc[(df_['ds'] > '2016-12-13') & (df_['ds'] < '2016-12-19'), 'y'] = None\n",
        "\n",
        "# Make monthly dataframe\n",
        "# df_monthly = df.copy()\n",
        "# df_monthly['ds'] = df_monthly['ds']+ pd.offsets.MonthBegin(0) \n",
        "# df_monthly = df_monthly.groupby(['ds'],as_index=False).agg('sum')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4s6vbo9bXK4c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_daily_forecast(df,\n",
        "#                           cap,\n",
        "                          holidays,\n",
        "                          growth,\n",
        "                          n_changepoints = 25,\n",
        "                          changepoint_prior_scale = 0.05,\n",
        "                          changepoints = None,\n",
        "                          holidays_prior_scale = 10,\n",
        "                          interval_width = 0.8,\n",
        "                          mcmc_samples = 1,\n",
        "                          future_num_points = 10):\n",
        "  \"\"\"\n",
        "  Create forecast\n",
        "  \"\"\"\n",
        "  \n",
        "  # Create copy of dataframe\n",
        "  df_ = df.copy()\n",
        "\n",
        "  # Add in growth parameter, which can change over time\n",
        "  #     df_['cap'] = max(df_['y']) if cap is None else cap\n",
        "\n",
        "  # Create model object and fit to dataframe\n",
        "  m = Prophet(growth=growth\n",
        "              , n_changepoints=n_changepoints\n",
        "              , changepoint_prior_scale=changepoint_prior_scale\n",
        "              , changepoints=changepoints\n",
        "              , holidays=holidays\n",
        "              , holidays_prior_scale=holidays_prior_scale\n",
        "              , interval_width=interval_width\n",
        "              , mcmc_samples=mcmc_samples)\n",
        "\n",
        "  # Fit model with dataframe\n",
        "  m.fit(df_)\n",
        "\n",
        "  # Create dataframe for predictions\n",
        "  future = m.make_future_dataframe(periods=future_num_points)\n",
        "  #     future['cap'] = max(df_['y']) if cap is None else cap\n",
        "\n",
        "  # Create predictions\n",
        "  fcst = m.predict(future)\n",
        "\n",
        "  # Plot\n",
        "  m.plot(fcst);\n",
        "  m.plot_components(fcst)\n",
        "\n",
        "  return fcst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HzVElZbNXN5s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fcst = create_daily_forecast(df_prophet,\n",
        "#                              cap,\n",
        "                             holidays,\n",
        "                             growth,\n",
        "                             n_changepoints,\n",
        "                             changepoint_prior_scale,\n",
        "                             changepoints, holidays_prior_scale,\n",
        "                             interval_width,\n",
        "                             mcmc_samples,\n",
        "                             future_num_points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ou05kS5eYNTX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calculate_mape(y_true, y_pred):\n",
        "    \"\"\" Calculate mean absolute percentage error (MAPE)\"\"\"\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "def calculate_mpe(y_true, y_pred):\n",
        "    \"\"\" Calculate mean percentage error (MPE)\"\"\"\n",
        "    return np.mean((y_true - y_pred) / y_true) * 100\n",
        "\n",
        "def calculate_mae(y_true, y_pred):\n",
        "    \"\"\" Calculate mean absolute error (MAE)\"\"\"\n",
        "    return np.mean(np.abs(y_true - y_pred)) * 100\n",
        "\n",
        "def calculate_rmse(y_true, y_pred):\n",
        "    \"\"\" Calculate root mean square error (RMSE)\"\"\"\n",
        "    return np.sqrt(np.mean((y_true - y_pred)**2))\n",
        "\n",
        "def print_error_metrics(y_true, y_pred):\n",
        "    print('MAPE: %f'%calculate_mape(y_true, y_pred))\n",
        "    print('MPE: %f'%calculate_mpe(y_true, y_pred))\n",
        "    print('MAE: %f'%calculate_mae(y_true, y_pred))\n",
        "    print('RMSE: %f'%calculate_rmse(y_true, y_pred))\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5-hzdzVMYWTI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print_error_metrics(y_true = df_prophet['y'], y_pred = fcst['yhat'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yrplxzAlTdBp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Let us model some time-series data! Finally! Bayesian structural time series modeling.\n",
        "\n",
        "We will not be going through an example here.  Below are some resources available to learn more about BSTS:\n",
        "\n",
        "-  [Causal Impact package from Google (available in R)](https://google.github.io/CausalImpact/CausalImpact.html).\n",
        "-  [Example implementation in python](https://www.analytics-link.com/single-post/2017/11/03/Causal-Impact-Analysis-in-R-and-now-Python).\n",
        "-  [Example implementation in python in github](https://github.com/yoonkanglow/Causal-Impact-Analysis-Demo/blob/master/causal_impact_analysis.ipynb)."
      ]
    },
    {
      "metadata": {
        "id": "zvI-0HR6brBZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Let us model some time-series data! Finally! LSTM for regression\n",
        "\n",
        "We will be going through an example here.\n",
        "\n",
        "Also, here are some resources on recurrent neural networks (RNN) and Long Short-Term Memory networks (LSTMs):\n",
        "* [Link 1](https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/)\n",
        "* [Link 2](https://blog.statsbot.co/time-series-prediction-using-recurrent-neural-networks-lstms-807fa6ca7f)\n",
        "* [Link 3](http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/)"
      ]
    },
    {
      "metadata": {
        "id": "6QITEMh2Zt4F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def do_lstm_model(df, \n",
        "                  ts, \n",
        "                  look_back, \n",
        "                  epochs, \n",
        "                  type_ = None, \n",
        "                  train_fraction = 0.67):\n",
        "  \"\"\"\n",
        "   Create LSTM model\n",
        "   Source: https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n",
        "  \"\"\"\n",
        "  # Import packages\n",
        "  import numpy\n",
        "  import matplotlib.pyplot as plt\n",
        "  from pandas import read_csv\n",
        "  import math\n",
        "  from keras.models import Sequential\n",
        "  from keras.layers import Dense\n",
        "  from keras.layers import LSTM\n",
        "  from sklearn.preprocessing import MinMaxScaler\n",
        "  from sklearn.metrics import mean_squared_error\n",
        "\n",
        "  # Convert an array of values into a dataset matrix\n",
        "  def create_dataset(dataset, look_back=1):\n",
        "    \"\"\"\n",
        "    Create the dataset\n",
        "    \"\"\"\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "      a = dataset[i:(i+look_back), 0]\n",
        "      dataX.append(a)\n",
        "      dataY.append(dataset[i + look_back, 0])\n",
        "    return numpy.array(dataX), numpy.array(dataY)\n",
        "\n",
        "  # Fix random seed for reproducibility\n",
        "  numpy.random.seed(7)\n",
        "\n",
        "  # Get dataset\n",
        "  dataset = df[ts].values\n",
        "  dataset = dataset.astype('float32')\n",
        "\n",
        "  # Normalize the dataset\n",
        "  scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "  dataset = scaler.fit_transform(dataset.reshape(-1, 1))\n",
        "  \n",
        "  # Split into train and test sets\n",
        "  train_size = int(len(dataset) * train_fraction)\n",
        "  test_size = len(dataset) - train_size\n",
        "  train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
        "  \n",
        "  # Reshape into X=t and Y=t+1\n",
        "  look_back = look_back\n",
        "  trainX, trainY = create_dataset(train, look_back)\n",
        "  testX, testY = create_dataset(test, look_back)\n",
        "  \n",
        "  # Reshape input to be [samples, time steps, features]\n",
        "  if type_ == 'regression with time steps':\n",
        "    trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
        "    testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
        "  elif type_ == 'stacked with memory between batches':\n",
        "    trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
        "    testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
        "  else:\n",
        "    trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "    testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "  \n",
        "  # Create and fit the LSTM network\n",
        "  batch_size = 1\n",
        "  model = Sequential()\n",
        "  \n",
        "  if type_ == 'regression with time steps':\n",
        "    model.add(LSTM(4, input_shape=(look_back, 1)))\n",
        "  elif type_ == 'memory between batches':\n",
        "    model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n",
        "  elif type_ == 'stacked with memory between batches':\n",
        "    model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True, return_sequences=True))\n",
        "    model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n",
        "  else:\n",
        "    model.add(LSTM(4, input_shape=(1, look_back)))\n",
        "  \n",
        "  model.add(Dense(1))\n",
        "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "  if type_ == 'memory between batches' or type_ == 'stacked with memory between batches':\n",
        "    for i in range(100):\n",
        "      model.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
        "      model.reset_states()\n",
        "  else:\n",
        "    model.fit(trainX, \n",
        "              trainY, \n",
        "              epochs = epochs, \n",
        "              batch_size = 1, \n",
        "              verbose = 2)\n",
        "  \n",
        "  # Make predictions\n",
        "  if type_ == 'memory between batches' or type_ == 'stacked with memory between batches':\n",
        "    trainPredict = model.predict(trainX, batch_size=batch_size)\n",
        "    testPredict = model.predict(testX, batch_size=batch_size)\n",
        "  else:\n",
        "    trainPredict = model.predict(trainX)\n",
        "    testPredict = model.predict(testX)\n",
        "  \n",
        "  # Invert predictions\n",
        "  trainPredict = scaler.inverse_transform(trainPredict)\n",
        "  trainY = scaler.inverse_transform([trainY])\n",
        "  testPredict = scaler.inverse_transform(testPredict)\n",
        "  testY = scaler.inverse_transform([testY])\n",
        "  \n",
        "  # Calculate root mean squared error\n",
        "  trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
        "  print('Train Score: %.2f RMSE' % (trainScore))\n",
        "  testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
        "  print('Test Score: %.2f RMSE' % (testScore))\n",
        "  \n",
        "  # Shift train predictions for plotting\n",
        "  trainPredictPlot = numpy.empty_like(dataset)\n",
        "  trainPredictPlot[:, :] = numpy.nan\n",
        "  trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
        "  \n",
        "  # Shift test predictions for plotting\n",
        "  testPredictPlot = numpy.empty_like(dataset)\n",
        "  testPredictPlot[:, :] = numpy.nan\n",
        "  testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
        "  \n",
        "  # Plot baseline and predictions\n",
        "  plt.plot(scaler.inverse_transform(dataset))\n",
        "  plt.plot(trainPredictPlot)\n",
        "  plt.plot(testPredictPlot)\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "  \n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xkZm9FWtaqF4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# LSTM Network for Regression\n",
        "do_lstm_model(df = df_prophet, \n",
        "              ts = 'y', \n",
        "              look_back = 1, \n",
        "              epochs = 5)\n",
        "\n",
        "# LSTM for Regression Using the Window Method\n",
        "do_lstm_model(df = df_prophet, \n",
        "              ts = 'y', \n",
        "              look_back = 3, \n",
        "              epochs = 5)\n",
        "\n",
        "# LSTM for Regression with Time Steps\n",
        "do_lstm_model(df = df_prophet, \n",
        "              ts = 'y', \n",
        "              look_back = 3, \n",
        "              epochs = 5, \n",
        "              type_ = 'regression with time steps')\n",
        "\n",
        "# LSTM with Memory Between Batches\n",
        "do_lstm_model(df = df_prophet, \n",
        "              ts = 'y', \n",
        "              look_back = 3, \n",
        "              epochs = 5, \n",
        "              type_ = 'memory between batches')\n",
        "\n",
        "# Stacked LSTMs with Memory Between Batches\n",
        "do_lstm_model(df = df_prophet, \n",
        "              ts = 'y', \n",
        "              look_back = 3, \n",
        "              epochs = 5, \n",
        "              type_ = 'stacked with memory between batches')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dAWZ9mx-msaJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# How do we evaluate the success of our models? How does this evaluation differ from traditional modeling tasks?  \n",
        "\n",
        "  We will not be going through an example here. We have looked at traditional contiuous evaluation modeling metrics, such as RMSE, MAPE, etc.  Here are some additional resources to learn about intricacies of time series model validation:\n",
        "* [Link on Quora](https://www.quora.com/Data-Science-Can-machine-learning-be-used-for-time-series-analysis)\n",
        "* [Another link](https://www.google.com/search?q=machine+learning+time+series+algorithms&oq=machine+learning+time+series+algorithms&aqs=chrome.0.0l4.5617j0j1&sourceid=chrome&ie=UTF-8)"
      ]
    },
    {
      "metadata": {
        "id": "VaL148Byb8hC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Appendix"
      ]
    },
    {
      "metadata": {
        "id": "7N_6HMoPTcEn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}